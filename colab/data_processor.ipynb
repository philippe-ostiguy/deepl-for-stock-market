{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7902c",
   "metadata": {},
   "source": [
    "\n",
    " The MIT License (MIT)\n",
    " Copyright (c) 2023 Philippe Ostiguy\n",
    "\n",
    " Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " of this software and associated documentation files (the \"Software\"), to deal\n",
    " in the Software without restriction, including without limitation the rights\n",
    " to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " copies of the Software, and to permit persons to whom the Software is\n",
    " furnished to do so, subject to the following conditions:\n",
    "\n",
    " The above copyright notice and this permission notice shall be included in\n",
    " all copies or substantial portions of the Software.\n",
    "\n",
    " THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    " EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    " MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
    " IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n",
    " DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n",
    " OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE\n",
    " OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45453a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from ratelimiter import RateLimiter\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float32)\n",
    "from typing import Optional, List, Tuple, Union, Dict, Any, Callable\n",
    "from dotenv import load_dotenv\n",
    "from copy import deepcopy\n",
    "from abc import ABC, abstractmethod\n",
    "from pmaw import PushshiftAPI\n",
    "import time\n",
    "import pytz\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import csv\n",
    "import requests\n",
    "import logging\n",
    "from contextlib import suppress\n",
    "import shutil\n",
    "from darts import TimeSeries\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "load_dotenv()\n",
    "\n",
    "from mean_reversion.config.config_utils import ConfigManager\n",
    "\n",
    "from mean_reversion.utils import (\n",
    "    read_json,\n",
    "    string_to_datetime,\n",
    "    est_dt_to_epoch,\n",
    "    get_previous_market_date,\n",
    "    create_file_if_not_exist,\n",
    "    obtain_market_dates,\n",
    "    read_csv_to_pd_formatted,\n",
    "    write_to_csv_formatted,\n",
    "    write_pd_to_csv\n",
    ")\n",
    "from mean_reversion.config.constants import (\n",
    "    RAW_ATTRIBUTES,\n",
    "    MODEL_PHASES,\n",
    "    MODEL_DATA_PATH,\n",
    "    ENGINEERED_DATA_TO_REMOVE,\n",
    "    DATASETS\n",
    ")\n",
    "\n",
    "\n",
    "class DataProcessorHelper:\n",
    "    def __init__(self, config_manager: ConfigManager):\n",
    "        self._config = config_manager.config\n",
    "        self._market_dates = obtain_market_dates(\n",
    "            start_date=self._config[\"common\"][\"start_date\"],\n",
    "            end_date=self._config[\"common\"][\"end_date\"],\n",
    "        )\n",
    "        self._obtain_split_index_set()\n",
    "\n",
    "    @property\n",
    "    def market_dates(self) -> pd.DataFrame:\n",
    "        return self._market_dates\n",
    "\n",
    "    @property\n",
    "    def data_to_write_ts(self) -> pd.DataFrame:\n",
    "        return self._data_to_write_ts\n",
    "\n",
    "    @data_to_write_ts.setter\n",
    "    def data_to_write_ts(self, value: pd.DataFrame) -> None:\n",
    "        self._data_to_write_ts = value\n",
    "\n",
    "    def remove_first_transformed_data(\n",
    "        self, data_to_process: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        return data_to_process.iloc[ENGINEERED_DATA_TO_REMOVE:]\n",
    "\n",
    "    def obtain_train_data(self, full_data_set: pd.DataFrame) -> pd.DataFrame:\n",
    "        return full_data_set.loc[: self._train_split_index]\n",
    "\n",
    "    def obtain_predict_data(self, full_data_set: pd.DataFrame) -> pd.DataFrame:\n",
    "        return full_data_set.loc[self._train_split_index + 1 : self._predict_split_index]\n",
    "\n",
    "    def obtain_test_data(self, full_data_set: pd.DataFrame) -> pd.DataFrame:\n",
    "        return full_data_set.loc[self._predict_split_index + 1:]\n",
    "\n",
    "    def _obtain_split_index_set(self) -> None:\n",
    "        total_length = len(self._market_dates)\n",
    "        self._train_split_index = int(total_length * self._config[\"common\"][\"train_test_split\"][0])\n",
    "        self._predict_split_index = self._train_split_index + int(total_length * self._config[\"common\"][\"train_test_split\"][1])\n",
    "\n",
    "    def write_ts_to_csv(\n",
    "        self, path_write_file: Dict, columns_to_drop: Optional[List] = None\n",
    "    ) -> None:\n",
    "        self._convert_to_ts_data(columns_to_drop)\n",
    "        self._write_ts_to_the_csv(path_write_file)\n",
    "\n",
    "    def _write_ts_to_the_csv(self, path_to_write_file: Dict) -> None:\n",
    "        write_to_csv_formatted(\n",
    "            self._train_time_series, path_to_write_file[\"train\"]\n",
    "        )\n",
    "\n",
    "        write_to_csv_formatted(\n",
    "            self._predict_time_series, path_to_write_file[\"predict\"]\n",
    "        )\n",
    "        write_to_csv_formatted(\n",
    "            self._test_time_series, path_to_write_file[\"test\"]\n",
    "        )\n",
    "\n",
    "    def _convert_to_ts_data(self, columns_to_drop: Optional[List]) -> None:\n",
    "        if columns_to_drop:\n",
    "            self._data_to_write_ts = self._data_to_write_ts.drop(\n",
    "                columns=columns_to_drop\n",
    "            )\n",
    "\n",
    "        self._train_time_series = TimeSeries.from_dataframe(\n",
    "            self.obtain_train_data(self._data_to_write_ts), freq=\"B\"\n",
    "        ).astype(np.float32)\n",
    "        self._predict_time_series = TimeSeries.from_dataframe(\n",
    "            self.obtain_predict_data(self._data_to_write_ts), freq=\"B\"\n",
    "        ).astype(np.float32)\n",
    "        self._test_time_series = TimeSeries.from_dataframe(\n",
    "            self.obtain_test_data(self._data_to_write_ts), freq=\"B\"\n",
    "        ).astype(np.float32)\n",
    "\n",
    "\n",
    "class DataForModelSelector:\n",
    "    def __init__(self, config_manager: ConfigManager):\n",
    "        self._config = config_manager.config\n",
    "        self._data_for_model_train = None\n",
    "        self._data_for_model_predict = None\n",
    "        self._data_for_model_test = None\n",
    "        self._datasets = DATASETS\n",
    "\n",
    "    def run(self) -> None:\n",
    "        last_data = {}\n",
    "        for dataset in self._datasets:\n",
    "            for sources in self._config[\"output\"]:\n",
    "                for data in sources['data']:\n",
    "                    if not os.path.exists(data[\"engineered\"][dataset]):\n",
    "                        continue\n",
    "                    if not hasattr(self, '_all_output'):\n",
    "                        self._all_output = read_csv_to_pd_formatted(data[\"engineered\"][dataset], sort_by_column_name='time')\n",
    "                        last_data = data\n",
    "                        continue\n",
    "                    new_data = read_csv_to_pd_formatted(data[\"engineered\"][dataset], 'time')\n",
    "                    self._all_output= self._all_output.merge(new_data, on='time', how='outer')\n",
    "                    last_data = data\n",
    "            if not last_data:\n",
    "                raise ValueError('No output file found')\n",
    "            write_to_csv_formatted(\n",
    "                self._all_output,\n",
    "                last_data[\"model_data\"][dataset],\n",
    "                'time')\n",
    "            del self._all_output\n",
    "\n",
    "\n",
    "        self._output_data_train = read_csv_to_pd_formatted(\n",
    "            last_data[\"model_data\"]['train'], \"time\"\n",
    "        )\n",
    "\n",
    "        for input_in_config in self._config[\"inputs\"][\"past_covariates\"][\n",
    "            \"sources\"\n",
    "        ]:\n",
    "            for current_input in input_in_config[\"data\"]:\n",
    "                train_file_path = current_input[\"engineered\"][\"train\"]\n",
    "                predict_file_path = current_input[\"engineered\"][\"predict\"]\n",
    "                test_file_path = current_input[\"engineered\"][\"test\"]\n",
    "                if not os.path.exists(train_file_path):\n",
    "                    continue\n",
    "                asset_name = current_input[\"asset\"]\n",
    "                data_train = self._load_and_prefix_data(\n",
    "                    train_file_path, asset_name, [\"time\"]\n",
    "                )\n",
    "\n",
    "                data_predict = self._load_and_prefix_data(\n",
    "                    predict_file_path, asset_name, [\"time\"]\n",
    "                )\n",
    "\n",
    "                data_test = self._load_and_prefix_data(\n",
    "                    test_file_path, asset_name, [\"time\"]\n",
    "                )\n",
    "\n",
    "                if not self._output_data_train[\"time\"].equals(\n",
    "                    data_train[\"time\"]\n",
    "                ):\n",
    "                    raise ValueError(\n",
    "                        f\"time values are not consistent in {asset_name}\"\n",
    "                    )\n",
    "\n",
    "                if self._data_for_model_train is None:\n",
    "                    self._data_for_model_train = data_train\n",
    "                    self._data_for_model_predict = data_predict\n",
    "                    self._data_for_model_test = data_test\n",
    "                else:\n",
    "                    duplicated_columns = (\n",
    "                        self._data_for_model_train.columns.intersection(\n",
    "                            data_train.columns\n",
    "                        )\n",
    "                    )\n",
    "                    duplicated_columns = [\n",
    "                        col for col in duplicated_columns if col != \"time\"\n",
    "                    ]\n",
    "                    if duplicated_columns:\n",
    "                        raise ValueError(\n",
    "                            f\"Duplicate columns found for asset {asset_name}: {', '.join(duplicated_columns)}\"\n",
    "                        )\n",
    "\n",
    "                    self._data_for_model_train = pd.merge(\n",
    "                        self._data_for_model_train,\n",
    "                        data_train,\n",
    "                        on=\"time\",\n",
    "                        how=\"left\",\n",
    "                    )\n",
    "                    self._data_for_model_predict = pd.merge(\n",
    "                        self._data_for_model_predict,\n",
    "                        data_predict,\n",
    "                        on=\"time\",\n",
    "                        how=\"left\",\n",
    "                    )\n",
    "                    self._data_for_model_test = pd.merge(\n",
    "                        self._data_for_model_test,\n",
    "                        data_test,\n",
    "                        on=\"time\",\n",
    "                        how=\"left\",\n",
    "                    )\n",
    "\n",
    "        if self._config[\"common\"][\"model_phase\"] == \"train\":\n",
    "            self._make_correlated_feature_removal()\n",
    "        else:\n",
    "            self._apply_correlated_feature_removal()\n",
    "\n",
    "        write_to_csv_formatted(\n",
    "            self._data_for_model_train,\n",
    "            self._config[\"inputs\"][\"past_covariates\"][\"common\"][\"model_data\"][\n",
    "                \"train\"\n",
    "            ],\n",
    "            'time'\n",
    "        )\n",
    "        write_to_csv_formatted(\n",
    "            self._data_for_model_predict,\n",
    "            self._config[\"inputs\"][\"past_covariates\"][\"common\"][\"model_data\"][\n",
    "                \"predict\"\n",
    "            ],\n",
    "            'time'\n",
    "        )\n",
    "        write_to_csv_formatted(\n",
    "            self._data_for_model_test,\n",
    "            self._config[\"inputs\"][\"past_covariates\"][\"common\"][\"model_data\"][\n",
    "                \"test\"\n",
    "            ],\n",
    "            'time'\n",
    "        )\n",
    "\n",
    "    def _make_correlated_feature_removal(self) -> None:\n",
    "        corr_matrix = self._data_for_model_train.corr().abs()\n",
    "        corr_sum = pd.DataFrame(\n",
    "            {\"sum\": corr_matrix.sum(), \"col\": corr_matrix.columns}\n",
    "        )\n",
    "        corr_sum_sorted = corr_sum.sort_values(\"sum\", ascending=False)\n",
    "\n",
    "        to_keep = [\"time\"]\n",
    "\n",
    "        for _, row in corr_sum_sorted.iterrows():\n",
    "            column = row[\"col\"]\n",
    "            if column == \"time\":\n",
    "                continue\n",
    "            if all(\n",
    "                corr_matrix[column][to_keep]\n",
    "                <= self._config[\"common\"][\"correlation_threshold\"]\n",
    "            ):\n",
    "                to_keep.append(column)\n",
    "\n",
    "        with open(\n",
    "            self._config[\"inputs\"][\"past_covariates\"][\"common\"][\"pickle\"][\n",
    "                \"features_to_keep\"\n",
    "            ],\n",
    "            \"wb\",\n",
    "        ) as f:\n",
    "            pickle.dump(to_keep, f)\n",
    "\n",
    "        self._raise_error_if_missing_columns(\n",
    "            to_keep, self._data_for_model_train, \"training data\"\n",
    "        )\n",
    "        self._raise_error_if_missing_columns(\n",
    "            to_keep, self._data_for_model_predict, \"prediction data\"\n",
    "        )\n",
    "        self._raise_error_if_missing_columns(\n",
    "            to_keep, self._data_for_model_test, \"test data\"\n",
    "        )\n",
    "\n",
    "        self._data_for_model_train = self._data_for_model_train[to_keep]\n",
    "        self._data_for_model_predict = self._data_for_model_predict[to_keep]\n",
    "        self._data_for_model_test= self._data_for_model_test[to_keep]\n",
    "\n",
    "    def _apply_correlated_feature_removal(self) -> None:\n",
    "        with open(\n",
    "            self._config[\"inputs\"][\"past_covariates\"][\"common\"][\"pickle\"][\n",
    "                \"features_to_keep\"\n",
    "            ],\n",
    "            \"rb\",\n",
    "        ) as f:\n",
    "            to_keep = pickle.load(f)\n",
    "        self._raise_error_if_missing_columns(\n",
    "            to_keep, self._data_for_model_predict, \"prediction data\"\n",
    "        )\n",
    "        self._raise_error_if_missing_columns(\n",
    "            to_keep, self._data_for_model_test, \"test data\"\n",
    "        )\n",
    "        self._data_for_model_predict = self._data_for_model_predict[to_keep]\n",
    "        self._data_for_model_train = self._data_for_model_train[to_keep]\n",
    "        self._data_for_model_test = self._data_for_model_test[to_keep]\n",
    "    @staticmethod\n",
    "    def _load_and_prefix_data(\n",
    "        file_path: str,\n",
    "        asset_name: str,\n",
    "        columns_not_to_change: Optional[List[str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        data = pd.read_csv(file_path)\n",
    "        if columns_not_to_change is not None:\n",
    "            columns_to_change = [\n",
    "                col for col in data.columns if col not in columns_not_to_change\n",
    "            ]\n",
    "            data_to_change = data[columns_to_change].add_prefix(\n",
    "                f\"{asset_name}_\"\n",
    "            )\n",
    "            data = pd.concat(\n",
    "                [data_to_change, data[columns_not_to_change]], axis=1\n",
    "            )\n",
    "        else:\n",
    "            data = data.add_prefix(f\"{asset_name}_\")\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _raise_error_if_missing_columns(\n",
    "        columns_to_keep: List[str], data: pd.DataFrame, data_name: str\n",
    "    ) -> None:\n",
    "        missing_columns = set(columns_to_keep) - set(data.columns)\n",
    "        if missing_columns:\n",
    "            raise KeyError(\n",
    "                f\"Columns not found in {data_name}: {missing_columns}\"\n",
    "            )\n",
    "\n",
    "\n",
    "class BaseInputOutputDataEngineering(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        processor: \"BaseDataProcessor\",\n",
    "        data_processor_helper: DataProcessorHelper,\n",
    "        columns_to_drop_ts: Optional[List] = RAW_ATTRIBUTES[0],\n",
    "    ):\n",
    "        self._processor = processor\n",
    "        self._data_processor_helper = data_processor_helper\n",
    "        self._engineered_data: pd.DataFrame = pd.DataFrame()\n",
    "        self._columns_to_drop_ts = columns_to_drop_ts\n",
    "        self._scaler_to_apply = StandardScaler\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_feature_engineering(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class InputDataEngineering(BaseInputOutputDataEngineering):\n",
    "    def __init__(\n",
    "        self,\n",
    "        processor: \"BaseDataProcessor\",\n",
    "        data_processor_helper: DataProcessorHelper,\n",
    "        data_for_stationarity_to_remove: Optional[\n",
    "            int\n",
    "        ] = ENGINEERED_DATA_TO_REMOVE,\n",
    "    ):\n",
    "        super().__init__(processor, data_processor_helper)\n",
    "\n",
    "        self._asset_name: str = \"\"\n",
    "        self._current_column: str = \"\"\n",
    "        self._data_for_stationarity_to_remove = data_for_stationarity_to_remove\n",
    "        self._transformations: List[\n",
    "            Tuple[str, Callable[[pd.Series], Optional[pd.Series]]]\n",
    "        ] = [\n",
    "            (\"identity\", self._make_identity),\n",
    "            (\"first_difference\", self._make_first_difference),\n",
    "            (\"logarithmic\", self._make_logarithmic),\n",
    "            (\"ratio\", self._make_ratio),\n",
    "        ]\n",
    "\n",
    "        self._inference_transformations: List[\n",
    "            Tuple[str, Callable[[pd.Series], pd.Series]]\n",
    "        ] = [\n",
    "            (\"identity\", self._apply_identity),\n",
    "            (\"ratio\", self._apply_ratio),\n",
    "            (\"first_difference\", self._apply_first_difference),\n",
    "            (\"logarithmic\", self._apply_logarithmic),\n",
    "\n",
    "        ]\n",
    "\n",
    "        self._stationarity_applied = {}\n",
    "        self._scaler_applied = {}\n",
    "\n",
    "    def _handle_missing_inf(self, data: pd.Series) -> Optional[pd.Series]:\n",
    "        missing_or_inf = data.isna() | np.isinf(data)\n",
    "        missing_or_inf_count = missing_or_inf.sum()\n",
    "        missing_indices = set(range(1, len(data) + 1)) - set(data.index)\n",
    "\n",
    "        if (missing_or_inf_count + len(missing_indices)) / len(\n",
    "            data\n",
    "        ) > self._processor._config[\"common\"][\"max_missing_data\"]:\n",
    "            return None\n",
    "\n",
    "        return self._handle_missing_inf_apply(data)\n",
    "\n",
    "    def _handle_missing_inf_apply(self, data: pd.Series) -> pd.Series:\n",
    "        missing_indices = set(range(1, len(data))) - set(data.index)\n",
    "        adjusted_data = data.copy()\n",
    "\n",
    "        if not 0 in missing_indices:\n",
    "            for index in missing_indices:\n",
    "                prev_index = max(i for i in adjusted_data.index if i < index)\n",
    "                adjusted_data = adjusted_data.reindex(range(1, index)).fillna(\n",
    "                    adjusted_data[prev_index]\n",
    "                )\n",
    "\n",
    "        first_valid_index = adjusted_data.first_valid_index()\n",
    "        adjusted_data.iloc[\n",
    "            : self._data_for_stationarity_to_remove\n",
    "        ] = adjusted_data[first_valid_index]\n",
    "\n",
    "        adjusted_data = adjusted_data.sort_index()\n",
    "        adjusted_data = adjusted_data.replace([np.inf, -np.inf], np.nan)\n",
    "        adjusted_data = adjusted_data.fillna(method=\"ffill\")\n",
    "\n",
    "        return adjusted_data\n",
    "\n",
    "    def _make_identity(self, data: pd.Series) -> Optional[pd.Series]:\n",
    "        data = self._handle_missing_inf(data)\n",
    "        return data\n",
    "\n",
    "    def _apply_identity(self, data: pd.Series) -> pd.Series:\n",
    "        data = self._handle_missing_inf_apply(data)\n",
    "        return data\n",
    "\n",
    "    def _make_ratio(self, data: pd.Series) -> Optional[pd.Series]:\n",
    "        ratio_data = data / data.shift(1)\n",
    "        ratio_data = self._handle_missing_inf(ratio_data)\n",
    "        return ratio_data\n",
    "\n",
    "    def _apply_ratio(self, data: pd.Series) -> pd.Series:\n",
    "        ratio_data = data / data.shift(1)\n",
    "        ratio_data = self._handle_missing_inf_apply(ratio_data)\n",
    "        return ratio_data\n",
    "\n",
    "    def _make_logarithmic(self, data: pd.Series) -> Optional[pd.Series]:\n",
    "        data = np.log(data + 1e-7)\n",
    "        data = self._handle_missing_inf(data)\n",
    "        return data\n",
    "\n",
    "    def _apply_logarithmic(self, data: pd.Series) -> pd.Series:\n",
    "        data = np.log(data + 1e-7)\n",
    "        data = self._handle_missing_inf_apply(data)\n",
    "        return data\n",
    "\n",
    "    def _make_first_difference(self, data: pd.Series) -> Optional[pd.Series]:\n",
    "        data_diff = data.diff()\n",
    "        data_diff = self._handle_missing_inf(data_diff)\n",
    "        return data_diff\n",
    "\n",
    "    def _apply_first_difference(self, data: pd.Series) -> pd.Series:\n",
    "        data_diff = data.diff()\n",
    "        data_diff = self._handle_missing_inf_apply(data_diff)\n",
    "        return data_diff\n",
    "\n",
    "    def _obtain_adf_p_value(self, time_series: pd.Series) -> float:\n",
    "        adf_result = adfuller(time_series)\n",
    "        return adf_result[1]\n",
    "\n",
    "    def coordinate_stationarity(self, data: pd.Series) -> Optional[str]:\n",
    "        valid_transformations = []\n",
    "        if not self._processor._config[\"common\"][\"make_data_stationary\"] :\n",
    "            return \"identity\"\n",
    "\n",
    "        for name, transform_function in self._transformations:\n",
    "            transformed_data = transform_function(data)\n",
    "            if transformed_data is None or (\n",
    "                    transformed_data.isna() | np.isinf(transformed_data)).any():\n",
    "                return None\n",
    "\n",
    "            if len(set(transformed_data)) == 1:\n",
    "                return None\n",
    "\n",
    "            p_value = self._obtain_adf_p_value(transformed_data)\n",
    "            if p_value < 0.05:\n",
    "                skewness = stats.skew(transformed_data)\n",
    "                kurtosis = stats.kurtosis(transformed_data)\n",
    "                if not self._processor._config[\"common\"][\"check_bell_shape\"]:\n",
    "                    valid_transformations.append(\n",
    "                        (name, p_value, skewness, kurtosis))\n",
    "\n",
    "                elif -2 <= skewness <= 7 and -2 <= kurtosis <= 7:\n",
    "                    valid_transformations.append(\n",
    "                        (name, p_value, skewness, kurtosis))\n",
    "        #self.TEMPO_FCT_MAKE_PLOT(data)\n",
    "        if not valid_transformations:\n",
    "            logging.warning(\n",
    "                f\"Asset {self._asset_name} and column '{self._current_column}' \"\n",
    "                f\"could not be made stationary with the given transformations.\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        for transformation in valid_transformations:\n",
    "            if transformation[0] == \"identity\":\n",
    "                return \"identity\"\n",
    "        final_transformation = min(valid_transformations, key=lambda x: (x[3], x[2]))[0]\n",
    "        return final_transformation\n",
    "\n",
    "    def TEMPO_FCT_MAKE_PLOT(self,data):\n",
    "        log_data = self._make_logarithmic(data)\n",
    "        first_diff = self._make_first_difference(data)\n",
    "        ratio_data = self._make_ratio(data)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(data)\n",
    "        plt.title('Original Series')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(log_data)\n",
    "        plt.title('Logarithm of Series')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Log(Value)')\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(first_diff)\n",
    "        plt.title('First Difference of Series')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('First Difference')\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(ratio_data)\n",
    "        plt.title('Ratio of Series')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Ratio')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show(block=False)\n",
    "        plt.close()\n",
    "\n",
    "    def run_feature_engineering(self):\n",
    "        self._engineered_data = read_csv_to_pd_formatted(\n",
    "            self._processor._config[\"data\"][\n",
    "                self._processor._current_data_index\n",
    "            ][\"preprocessed\"]\n",
    "        )\n",
    "        self._columns_to_transform = [\n",
    "            column\n",
    "            for column in self._engineered_data.columns\n",
    "            if column != RAW_ATTRIBUTES[0]\n",
    "        ]\n",
    "        self._coordinate_engineering_methods()\n",
    "        self._dispatch_feature_engineering(\"data_stationary\")\n",
    "        self._engineered_data = (\n",
    "            self._data_processor_helper.remove_first_transformed_data(\n",
    "                self._engineered_data\n",
    "            )\n",
    "        )\n",
    "        self._transformed_columns = [ col for col in\n",
    "                                      self._engineered_data.columns.tolist()\n",
    "                                      if col != RAW_ATTRIBUTES[0]]\n",
    "\n",
    "        if self._transformed_columns :\n",
    "            if self._processor._config[\"common\"][\"scaling\"]:\n",
    "                self._dispatch_feature_engineering(\"data_scaling\")\n",
    "            self._data_processor_helper.data_to_write_ts = self._engineered_data\n",
    "\n",
    "            self._data_processor_helper.write_ts_to_csv(\n",
    "                self._processor._config[\"data\"][\n",
    "                    self._processor._current_data_index\n",
    "                ][\"engineered\"],\n",
    "                self._columns_to_drop_ts,\n",
    "            )\n",
    "\n",
    "            self._perform_pickle_operation(\n",
    "                self._processor._config[\"common\"][\"pickle\"][\n",
    "                    \"transformations_for_scaler\"\n",
    "                ],\n",
    "                \"wb\",\n",
    "                self._scaler_applied,\n",
    "            )\n",
    "            self._perform_pickle_operation(\n",
    "                self._processor._config[\"common\"][\"pickle\"][\n",
    "                    \"transformations_for_stationarity\"\n",
    "                ],\n",
    "                \"wb\",\n",
    "                self._stationarity_applied,\n",
    "            )\n",
    "\n",
    "    def _dispatch_feature_engineering(self, operation):\n",
    "        if (\n",
    "            self._processor._config[\"common\"][\"model_phase\"]\n",
    "            == self._processor._model_phase[0]\n",
    "        ):\n",
    "            getattr(self, f\"_make_{operation}\")()\n",
    "        else:\n",
    "            getattr(self, f\"_apply_{operation}\")()\n",
    "\n",
    "    def _obtain_pickle_inference(self, file: str) -> Dict[str, Any]:\n",
    "        dictionary = self._perform_pickle_operation(file, \"rb\")\n",
    "        return dictionary.get(\n",
    "            self._processor._config[\"data\"][\n",
    "                self._processor._current_data_index\n",
    "            ][\"asset\"]\n",
    "        )\n",
    "\n",
    "    def _perform_pickle_operation(\n",
    "        self,\n",
    "        filepath: str,\n",
    "        operation: str,\n",
    "        transformation: Optional[Union[Dict, StandardScaler]] = None,\n",
    "    ) -> Any:\n",
    "        with open(filepath, operation) as file:\n",
    "            if operation == \"rb\":\n",
    "                with suppress(\n",
    "                    (\n",
    "                        pickle.PicklingError,\n",
    "                        pickle.UnpicklingError,\n",
    "                        FileNotFoundError,\n",
    "                        Exception,\n",
    "                    )\n",
    "                ):\n",
    "                    return pickle.load(file)\n",
    "            elif operation == \"wb\":\n",
    "                assert (\n",
    "                    transformation is not None\n",
    "                ), \"Cannot write None transformation\"\n",
    "                with suppress(\n",
    "                    (\n",
    "                        pickle.PicklingError,\n",
    "                        pickle.UnpicklingError,\n",
    "                        FileNotFoundError,\n",
    "                        Exception,\n",
    "                    )\n",
    "                ):\n",
    "                    try:\n",
    "                        existing_data = pickle.load(file)\n",
    "                    except (\n",
    "                        pickle.PicklingError,\n",
    "                        pickle.UnpicklingError,\n",
    "                        FileNotFoundError,\n",
    "                        Exception,\n",
    "                    ):\n",
    "                        existing_data = {}\n",
    "\n",
    "                    existing_data.update(transformation)\n",
    "                    pickle.dump(existing_data, file)\n",
    "\n",
    "    def _make_data_scaling(self) -> None:\n",
    "        self._scaler_applied[\n",
    "            self._processor._config[\"data\"][\n",
    "                self._processor._current_data_index\n",
    "            ][\"asset\"]\n",
    "        ] = {}\n",
    "        train_data = self._data_processor_helper.obtain_train_data(\n",
    "            self._engineered_data\n",
    "        ).copy()\n",
    "        predict_data = self._data_processor_helper.obtain_predict_data(\n",
    "            self._engineered_data\n",
    "        ).copy()\n",
    "        test_data = self._data_processor_helper.obtain_test_data(\n",
    "            self._engineered_data\n",
    "        ).copy()\n",
    "\n",
    "        for column in self._transformed_columns:\n",
    "            scaler = self._scaler_to_apply()\n",
    "            train_data[column] = scaler.fit_transform(train_data[[column]])\n",
    "            predict_data[column] = scaler.transform(predict_data[[column]])\n",
    "            test_data[column] = scaler.transform(test_data[[column]])\n",
    "            self._scaler_applied[\n",
    "                self._processor._config[\"data\"][\n",
    "                    self._processor._current_data_index\n",
    "                ][\"asset\"]\n",
    "            ][column] = scaler\n",
    "\n",
    "        self._engineered_data = pd.concat([train_data, predict_data, test_data])\n",
    "\n",
    "    def _perform_common_not_stationary(self, column: str) -> None:\n",
    "        self._engineered_data = self._engineered_data.drop(column, axis=1)\n",
    "\n",
    "\n",
    "    def _apply_data_scaling(self) -> None:\n",
    "        self._scaler_applied = self._obtain_pickle_inference(\n",
    "            self._processor._config[\"common\"][\"pickle\"][\n",
    "                \"transformations_for_scaler\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if self._scaler_applied is not None:\n",
    "            for column in self._transformed_columns:\n",
    "                if column in self._scaler_applied:\n",
    "                    self._engineered_data[column] = self._scaler_applied[\n",
    "                        column\n",
    "                    ].transform(self._engineered_data[[column]])\n",
    "\n",
    "    def _apply_data_stationary(self) -> None:\n",
    "        self._stationarity_applied = self._obtain_pickle_inference(\n",
    "            self._processor._config[\"common\"][\"pickle\"][\n",
    "                \"transformations_for_stationarity\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if self._stationarity_applied is not None:\n",
    "            for column in self._columns_to_transform:\n",
    "                transformation_name = self._stationarity_applied[column]\n",
    "                if transformation_name == \"not stationary\":\n",
    "                    self._perform_common_not_stationary(column)\n",
    "                else:\n",
    "                    transformation_func_dict = dict(\n",
    "                        self._inference_transformations\n",
    "                    )\n",
    "                    transformation_func = transformation_func_dict[\n",
    "                        transformation_name\n",
    "                    ]\n",
    "                    self._engineered_data[column] = transformation_func(\n",
    "                        self._engineered_data[column]\n",
    "                    )\n",
    "\n",
    "    def _make_data_stationary(self) -> None:\n",
    "        self._asset_name = self._processor._config[\"data\"][\n",
    "            self._processor._current_data_index\n",
    "        ][\"asset\"]\n",
    "        self._stationarity_applied[\n",
    "            self._processor._config[\"data\"][\n",
    "                self._processor._current_data_index\n",
    "            ][\"asset\"]\n",
    "        ] = {}\n",
    "\n",
    "        train_data = self._data_processor_helper.obtain_train_data(\n",
    "            self._engineered_data\n",
    "        )\n",
    "        predict_data = self._data_processor_helper.obtain_predict_data(\n",
    "            self._engineered_data\n",
    "        )\n",
    "        test_data = self._data_processor_helper.obtain_test_data(\n",
    "            self._engineered_data\n",
    "        )\n",
    "\n",
    "        for column in self._columns_to_transform:\n",
    "            self._current_column = column\n",
    "            best_transformation = self.coordinate_stationarity(\n",
    "                train_data[column]\n",
    "            )\n",
    "\n",
    "            if best_transformation is None:\n",
    "                self._perform_common_not_stationary(column)\n",
    "                self._stationarity_applied[\n",
    "                    self._processor._config[\"data\"][\n",
    "                        self._processor._current_data_index\n",
    "                    ][\"asset\"]\n",
    "                ][column] = \"not stationary\"\n",
    "            else:\n",
    "                self._stationarity_applied[\n",
    "                    self._processor._config[\"data\"][\n",
    "                        self._processor._current_data_index\n",
    "                    ][\"asset\"]\n",
    "                ][column] = best_transformation\n",
    "                self._engineered_data[column] = dict(self._transformations)[\n",
    "                    best_transformation\n",
    "                ](pd.concat([train_data[column], predict_data[column], test_data[column]]))\n",
    "\n",
    "    def _coordinate_engineering_methods(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OutputDataEngineering(BaseInputOutputDataEngineering):\n",
    "    def __init__(\n",
    "        self,\n",
    "        processor: \"BaseDataProcessor\",\n",
    "        data_processor_helper: DataProcessorHelper,\n",
    "        data_model_dir: Optional[str] = MODEL_DATA_PATH,\n",
    "    ):\n",
    "        super().__init__(processor, data_processor_helper)\n",
    "        self._data_model_dir = data_model_dir\n",
    "\n",
    "    def run_feature_engineering(self):\n",
    "        self._engineered_data = read_csv_to_pd_formatted(\n",
    "            self._processor._config[\"data\"][\n",
    "                self._processor._current_data_index\n",
    "            ][\"preprocessed\"]\n",
    "        )\n",
    "\n",
    "        target_column = (self._processor._config[\"data\"][self._processor._current_data_index]['asset'] + '_target').lower()\n",
    "        if not self._processor._config[\"common\"][\"make_data_stationary\"]:\n",
    "            if '4. close' in self._engineered_data.columns:\n",
    "                self._engineered_data[target_column] = self._engineered_data['4. close']\n",
    "            elif 'value' in self._engineered_data.columns:\n",
    "                self._engineered_data[target_column] = self._engineered_data['value']\n",
    "\n",
    "            else :\n",
    "                raise ValueError('Columns most have open and close name or value name')\n",
    "\n",
    "        if '1. open' in self._engineered_data.columns and '4. close' in self._engineered_data.columns:\n",
    "            self._engineered_data[target_column] = (\n",
    "            self._engineered_data['4. close']\n",
    "            / self._engineered_data[\"1. open\"]\n",
    "        ) - 1\n",
    "\n",
    "        elif 'value' in self._engineered_data.columns :\n",
    "            self._engineered_data[target_column] \\\n",
    "                = (self._engineered_data['value']/\n",
    "                   self._engineered_data[\"value\"].shift(1)) - 1\n",
    "            self._engineered_data.iloc[0] = 0\n",
    "        else :\n",
    "            raise ValueError('Columns most have open and close name or value name')\n",
    "\n",
    "        self._engineered_data =\\\n",
    "            self._data_processor_helper.remove_first_transformed_data(\n",
    "                self._engineered_data\n",
    "            )\n",
    "        columns_to_keep = {target_column}\n",
    "        self._engineered_data = self._engineered_data[[*columns_to_keep]]\n",
    "\n",
    "        train_data = self._data_processor_helper.obtain_train_data(\n",
    "            self._engineered_data\n",
    "        ).copy()\n",
    "        predict_data = self._data_processor_helper.obtain_predict_data(\n",
    "            self._engineered_data\n",
    "        ).copy()\n",
    "\n",
    "        test_data = self._data_processor_helper.obtain_test_data(\n",
    "            self._engineered_data\n",
    "        ).copy()\n",
    "\n",
    "        self._data_processor_helper.data_to_write_ts = \\\n",
    "            pd.concat([train_data[list(columns_to_keep)[0]],\n",
    "                       predict_data[list(columns_to_keep)[0]],\n",
    "                       test_data[list(columns_to_keep)[0]]]).to_frame()\n",
    "        self._data_processor_helper.write_ts_to_csv(\n",
    "            self._processor._config[\"data\"][\n",
    "                self._processor._current_data_index\n",
    "            ][\"engineered\"]\n",
    "        )\n",
    "\n",
    "\n",
    "class BaseDataProcessor(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        data_processor_helper: DataProcessorHelper,\n",
    "        is_input_feature: Optional[bool] = True,\n",
    "        model_phase: Optional[str] = MODEL_PHASES,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._config = config\n",
    "        self._model_phase = model_phase\n",
    "        self._is_current_asset_dropped = False\n",
    "        self._attributes_to_delete = read_json(\n",
    "            \"resources/configs/models_support.json\"\n",
    "        )[\"attributes_to_discard\"]\n",
    "        if is_input_feature:\n",
    "            pass\n",
    "            self._feature_engineering_strategy = InputDataEngineering(\n",
    "                self, data_processor_helper\n",
    "            )\n",
    "        else:\n",
    "            self._feature_engineering_strategy = OutputDataEngineering(\n",
    "                self, data_processor_helper\n",
    "            )\n",
    "\n",
    "\n",
    "    def _run_common_fetch(self):\n",
    "        start_date = self._config[\"common\"][\"start_date\"]\n",
    "        end_date = self._config[\"common\"][\"end_date\"]\n",
    "        write_new_data = False\n",
    "        try :\n",
    "            self._raw_data = read_csv_to_pd_formatted(self._config[\"data\"][self._current_data_index][\"raw\"])\n",
    "        except FileNotFoundError :\n",
    "            self._raw_data = self._run_fetch()\n",
    "            write_pd_to_csv(data=self._raw_data,\n",
    "                            file=self._config[\"data\"][self._current_data_index][\n",
    "                                \"raw\"])\n",
    "            return\n",
    "\n",
    "        pd_datetime = pd.to_datetime(self._raw_data['date'])\n",
    "        first_date = pd_datetime.iloc[0]\n",
    "        last_date = pd_datetime.iloc[-1]\n",
    "\n",
    "\n",
    "        if first_date > pd.to_datetime(self._config[\"common\"][\"start_date\"]):\n",
    "            self._config[\"common\"][\"end_date\"] = (first_date - pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            self._raw_data = pd.concat([self._run_fetch(),self._raw_data],ignore_index=True)\n",
    "            write_new_data = True\n",
    "        if last_date < pd.to_datetime(self._config[\"common\"][\"end_date\"]):\n",
    "            self._config[\"common\"][\"start_date\"] = (last_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            self._raw_data = pd.concat([self._run_fetch(), self._raw_data],\n",
    "                                       ignore_index=True)\n",
    "            write_new_data = True\n",
    "\n",
    "        if write_new_data :\n",
    "            self._raw_data.drop_duplicates(subset=[RAW_ATTRIBUTES[0]], keep='first', inplace=True)\n",
    "            write_pd_to_csv(data=self._raw_data,file=self._config[\"data\"][self._current_data_index][\"raw\"])\n",
    "            self._config[\"common\"][\"start_date\"] = start_date\n",
    "            self._config[\"common\"][\"end_date\"] = end_date\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def _run_fetch(self) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        assets_to_remove = []\n",
    "        for index, _ in enumerate(self._config[\"data\"]):\n",
    "            self._current_data_index = index\n",
    "            self._is_current_asset_dropped = False\n",
    "\n",
    "            self._execute_if_not_dropped(self._run_common_fetch,\n",
    "                condition=self._config[\"common\"][\"fetching\"]\n",
    "            )\n",
    "            self._execute_if_not_dropped(\n",
    "                self._run_preprocess,\n",
    "                condition=self._config[\"common\"][\"preprocessing\"]\n",
    "            )\n",
    "            self._execute_if_not_dropped(\n",
    "                self._feature_engineering_strategy.run_feature_engineering,\n",
    "                condition=self._config[\"common\"][\"engineering\"]\n",
    "            )\n",
    "            if self._is_current_asset_dropped:\n",
    "                assets_to_remove.append(index)\n",
    "\n",
    "        for index in sorted(assets_to_remove, reverse=True):\n",
    "            del self._config[\"data\"][index]\n",
    "\n",
    "    def _execute_if_not_dropped(\n",
    "        self, method: Callable, *args, condition: Optional[bool] =True, **kwargs\n",
    "    ) -> None:\n",
    "        if condition and not self._is_current_asset_dropped:\n",
    "            method(*args,**kwargs)\n",
    "\n",
    "    def _run_preprocess(self):\n",
    "        self._preprocessed_data = read_csv_to_pd_formatted(\n",
    "            self._config[\"data\"][self._current_data_index][\"raw\"]\n",
    "        )\n",
    "        self._preprocessed_data = self._preprocessed_data.rename(\n",
    "            columns=str.lower\n",
    "        )\n",
    "        self._preprocessed_data = self._make_attribute_index(\n",
    "            self._preprocessed_data\n",
    "        )\n",
    "        self._obtain_data_within_date()\n",
    "        self._keep_trading_day()\n",
    "        self._remove_attributes_in_data()\n",
    "        self._clean_data()\n",
    "        self._preprocessed_data = self._handle_missing_data(\n",
    "            self._preprocessed_data\n",
    "        )\n",
    "        self._execute_if_not_dropped(write_to_csv_formatted,\n",
    "            self._preprocessed_data,\n",
    "            self._config[\"data\"][self._current_data_index][\"preprocessed\"],\n",
    "        )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_attribute_index(\n",
    "        data: pd.DataFrame, attribute: Optional[str] = RAW_ATTRIBUTES[0]\n",
    "    ) -> pd.DataFrame:\n",
    "        return data.set_index(\n",
    "            pd.to_datetime(data[attribute]), inplace=False, drop=False\n",
    "        )\n",
    "\n",
    "    def _remove_attributes_in_data(self) -> None:\n",
    "        attributes_to_remove = [\n",
    "            item.lower() for item in self._attributes_to_delete\n",
    "        ]\n",
    "\n",
    "        columns_to_drop = []\n",
    "        for column in self._preprocessed_data.columns:\n",
    "            for attribute_to_remove in attributes_to_remove:\n",
    "                if attribute_to_remove in column.lower():\n",
    "                    columns_to_drop.append(column)\n",
    "\n",
    "        self._preprocessed_data = self._preprocessed_data.drop(\n",
    "            columns=columns_to_drop\n",
    "        )\n",
    "\n",
    "    def _clean_data(self) -> None:\n",
    "        mask = self._preprocessed_data.isin([\"\", \".\", None])\n",
    "        rows_to_remove = mask.any(axis=1)\n",
    "        self._preprocessed_data = self._preprocessed_data.loc[~rows_to_remove]\n",
    "\n",
    "    def _keep_trading_day(self) -> None:\n",
    "        market_dates = obtain_market_dates(\n",
    "            start_date=self._config[\"common\"][\"start_date\"],\n",
    "            end_date=self._config[\"common\"][\"end_date\"],\n",
    "        )\n",
    "        self._preprocessed_data = self._preprocessed_data.loc[\n",
    "            self._preprocessed_data.index.isin(market_dates.index)\n",
    "        ]\n",
    "\n",
    "    def _obtain_data_within_date(\n",
    "        self, date_column: Optional[str] = RAW_ATTRIBUTES[0]\n",
    "    ) -> None:\n",
    "        self._preprocessed_data = self._preprocessed_data[\n",
    "            (\n",
    "                self._preprocessed_data[date_column]\n",
    "                >= self._config[\"common\"][\"start_date\"]\n",
    "            )\n",
    "            & (\n",
    "                self._preprocessed_data[date_column]\n",
    "                <= self._config[\"common\"][\"end_date\"]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def _insert_missing_date(\n",
    "        self, data: pd.DataFrame, date: str, column: str\n",
    "    ) -> pd.DataFrame:\n",
    "        if date not in data[column].values:\n",
    "            prev_date = (\n",
    "                data[data[column] < date].iloc[-1]\n",
    "                if not data[data[column] < date].empty\n",
    "                else data.iloc[0]\n",
    "            )\n",
    "            new_row = prev_date.copy()\n",
    "            new_row[column] = date\n",
    "            data = (\n",
    "                pd.concat([data, new_row.to_frame().T], ignore_index=True)\n",
    "                .sort_values(by=column)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "        return data\n",
    "\n",
    "    def _handle_missing_data(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        threshold: Optional[int] = 1,\n",
    "        column: Optional[str] = RAW_ATTRIBUTES[0],\n",
    "    ) -> Union[None,pd.DataFrame]:\n",
    "        modified_data = data.copy()\n",
    "        market_open_dates = obtain_market_dates(\n",
    "            start_date=self._config[\"common\"][\"start_date\"],\n",
    "            end_date=self._config[\"common\"][\"end_date\"],\n",
    "        )\n",
    "\n",
    "        market_open_dates[\"count\"] = 0\n",
    "        market_open_dates.index = market_open_dates.index.strftime(\"%Y-%m-%d\")\n",
    "        date_counts = data[column].value_counts()\n",
    "\n",
    "        market_open_dates[\"count\"] = market_open_dates.index.map(\n",
    "            date_counts\n",
    "        ).fillna(0)\n",
    "\n",
    "        missing_dates = market_open_dates.loc[\n",
    "            market_open_dates[\"count\"] < threshold\n",
    "        ]\n",
    "\n",
    "        if not missing_dates.empty:\n",
    "            max_count = (\n",
    "                len(market_open_dates)\n",
    "                * self._config[\"common\"][\"max_missing_data\"]\n",
    "            )\n",
    "\n",
    "            if len(missing_dates) > max_count:\n",
    "                logging.warning(\n",
    "                    f\"For current asset {self._config['data'][self._current_data_index]['asset']},there are \"\n",
    "                    f\"{len(missing_dates)} missing data which is than the maximum threshold of \"\n",
    "                    f\"{self._config['common']['max_missing_data'] * 100}%\"\n",
    "                )\n",
    "                self._drop_current_asset()\n",
    "                return None\n",
    "            else:\n",
    "                for date, row in missing_dates.iterrows():\n",
    "                    modified_data = self._insert_missing_date(\n",
    "                        modified_data, date, column\n",
    "                    )\n",
    "        return modified_data\n",
    "\n",
    "    def _drop_current_asset(self):\n",
    "        self._is_current_asset_dropped = True\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        text = text.strip()\n",
    "        return re.sub(\" +\", \" \", text)\n",
    "\n",
    "\n",
    "class FutureCovariatesProcessor:\n",
    "    def __init__(self, config_manager, data_processor_helper):\n",
    "        self._config = config_manager.config\n",
    "        self._data_processor_helper = data_processor_helper\n",
    "        self._data_processor_helper.market_dates[\n",
    "            RAW_ATTRIBUTES[0]\n",
    "        ] = self._data_processor_helper.market_dates.index\n",
    "        self._data_processor_helper.market_dates.index = pd.RangeIndex(\n",
    "            start=0, stop=len(self._data_processor_helper.market_dates)\n",
    "        )\n",
    "        self._future_covariates = (\n",
    "            self._data_processor_helper.remove_first_transformed_data(\n",
    "                self._data_processor_helper.market_dates.copy()\n",
    "            )\n",
    "        )\n",
    "        self._train_data = self._data_processor_helper.obtain_train_data(\n",
    "            self._future_covariates.copy()\n",
    "        )\n",
    "        self._predict_data = self._data_processor_helper.obtain_predict_data(\n",
    "            self._future_covariates.copy()\n",
    "        )\n",
    "        self._test_data = self._data_processor_helper.obtain_test_data(\n",
    "            self._future_covariates.copy()\n",
    "        )\n",
    "        self._scaler = {}\n",
    "\n",
    "    def run(self):\n",
    "        self._train_data = self._add_covariates(\n",
    "            self._train_data.copy(), fit=True\n",
    "        )\n",
    "        self._predict_data = self._add_covariates(\n",
    "            self._predict_data.copy(), fit=False\n",
    "        )\n",
    "        self._test_data = self._add_covariates(\n",
    "            self._test_data.copy(), fit=False\n",
    "        )\n",
    "        self._data_processor_helper.data_to_write_ts = pd.concat(\n",
    "            [self._train_data, self._predict_data, self._test_data]\n",
    "        )[self._config[\"inputs\"][\"future_covariates\"][\"data\"]]\n",
    "        self._data_processor_helper.write_ts_to_csv(\n",
    "            self._config[\"inputs\"][\"future_covariates\"][\"common\"][\"model_data\"]\n",
    "        )\n",
    "\n",
    "    def _add_covariates(self, df: pd.DataFrame, fit: bool) -> pd.DataFrame:\n",
    "        covariates = self._config[\"inputs\"][\"future_covariates\"][\"data\"]\n",
    "        if covariates and \"day\" in covariates:\n",
    "            df[\"day\"] = df[RAW_ATTRIBUTES[0]].dt.day\n",
    "            df[\"day\"] = self._scale(\"day\", df[\"day\"], fit)\n",
    "        if \"month\" in covariates:\n",
    "            df[\"month\"] = df[RAW_ATTRIBUTES[0]].dt.month\n",
    "            df[\"month\"] = self._scale(\"month\", df[\"month\"], fit)\n",
    "        return df\n",
    "\n",
    "    def _scale(self, feature: str, data: pd.Series, fit: bool) -> pd.Series:\n",
    "\n",
    "        if not self._config[\"common\"][\"scaling\"]:\n",
    "            return data\n",
    "        data_reshaped = data.values.reshape(-1, 1)\n",
    "        if fit :\n",
    "            self._scaler[feature] = MinMaxScaler()\n",
    "            self._scaler[feature].fit(data_reshaped)\n",
    "            return self._scaler[feature].transform(data_reshaped)\n",
    "        else :\n",
    "            return self._scaler[feature].transform(data_reshaped)\n",
    "\n",
    "class Reddit(BaseDataProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        specific_config,\n",
    "        data_processor_helper,\n",
    "        is_input_feature=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            specific_config, data_processor_helper, is_input_feature, **kwargs\n",
    "        )\n",
    "        self._start_date_unix = est_dt_to_epoch(\n",
    "            string_to_datetime(self._config[\"common\"][\"start_date\"]).replace(\n",
    "                hour=9, minute=0, second=0\n",
    "            )\n",
    "        )\n",
    "        end_date_dt = (\n",
    "            string_to_datetime(self._config[\"common\"][\"end_date\"])\n",
    "            + timedelta(days=1)\n",
    "        ).replace(hour=9, minute=0, second=0)\n",
    "        self._current_date = self._config[\"common\"][\"end_date\"]\n",
    "        self._end_date_unix = est_dt_to_epoch(end_date_dt)\n",
    "        self._comments = []\n",
    "        self._push_shift = PushshiftAPI()\n",
    "\n",
    "    def _run_fetch(self):\n",
    "        while self._start_date_unix < self._end_date_unix:\n",
    "            self._obtain_data()\n",
    "            self._filter_raw_data()\n",
    "            self._save_data_reddit()\n",
    "            self._end_date_unix -= 86400\n",
    "            self._end_date_unix = self._check_and_adjust_unix_timestamp(\n",
    "                self._end_date_unix\n",
    "            )\n",
    "            dt = datetime.strptime(self._current_date, \"%Y-%m-%d\")\n",
    "            new_dt = dt - timedelta(days=1)\n",
    "            self._current_date = new_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    def _run_preprocess(self):\n",
    "        self._preprocessed_data = read_csv_to_pd_formatted(\n",
    "            (self._config[\"data\"][self._current_data_index][\"raw\"]),\n",
    "            \"created_utc\",\n",
    "        )\n",
    "        self._obtain_data_within_date()\n",
    "        self._keep_valid_market_data_reddit()\n",
    "        self._clean_reddit_data()\n",
    "        self._preprocessed_data = self._handle_missing_data(\n",
    "            self._preprocessed_data,\n",
    "            threshold=self._config[\"data\"][self._current_data_index][\"size\"],\n",
    "        )\n",
    "        write_to_csv_formatted(\n",
    "            self._preprocessed_data,\n",
    "            self._config[\"data\"][self._current_data_index][\"preprocessed\"],\n",
    "        )\n",
    "\n",
    "    def _keep_valid_market_data_reddit(self):\n",
    "        self._preprocessed_data[RAW_ATTRIBUTES[0]] = pd.to_datetime(\n",
    "            self._preprocessed_data[RAW_ATTRIBUTES[0]]\n",
    "        )\n",
    "        last_market_date = None\n",
    "        self._preprocessed_data[RAW_ATTRIBUTES[0]], last_market_date = zip(\n",
    "            *self._preprocessed_data[RAW_ATTRIBUTES[0]].apply(\n",
    "                lambda x: get_previous_market_date(\n",
    "                    x, last_market_date=last_market_date\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self._preprocessed_data[RAW_ATTRIBUTES[0]] = self._preprocessed_data[\n",
    "            RAW_ATTRIBUTES[0]\n",
    "        ].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    def _keep_unique_values(self, data) -> List[dict]:\n",
    "        seen = set()\n",
    "        unique_data = []\n",
    "        for row in data:\n",
    "            key = (row[\"created_utc\"], row[RAW_ATTRIBUTES[1]])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_data.append(row)\n",
    "        return unique_data\n",
    "\n",
    "    def _clean_reddit_data(self):\n",
    "        self._preprocessed_data = self._preprocessed_data[\n",
    "            self._preprocessed_data[RAW_ATTRIBUTES[1]].notnull()\n",
    "            & (self._preprocessed_data[RAW_ATTRIBUTES[1]].str.strip() != \"\")\n",
    "        ]\n",
    "        self._preprocessed_data = self._preprocessed_data.drop_duplicates()\n",
    "        self._preprocessed_data = self._preprocessed_data.drop_duplicates(\n",
    "            subset=[\"created_utc\", RAW_ATTRIBUTES[1]]\n",
    "        )\n",
    "        self._preprocessed_data[RAW_ATTRIBUTES[1]] = self._preprocessed_data[\n",
    "            RAW_ATTRIBUTES[1]\n",
    "        ].apply(self._clean_text)\n",
    "\n",
    "    def _coordinate_engineering_methods(self):\n",
    "        self._coordinate_roberta_engineer()\n",
    "\n",
    "    def _coordinate_roberta_engineer(self):\n",
    "        # IL FAUT RETRAVAILLER CETTE METHODE. IL NE FAUT PAS SAUVEGARGER LES DONNÉES\n",
    "        # DANS UN CSV, MAIS UTILISER UN PANDAS\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "        )\n",
    "        self._model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "        )\n",
    "\n",
    "        with open(\n",
    "            self._config[\"data\"][self._current_data_index][\"engineered\"],\n",
    "            \"w\",\n",
    "            encoding=\"utf-8\",\n",
    "            newline=\"\",\n",
    "        ) as f:\n",
    "            writer = csv.DictWriter(\n",
    "                f,\n",
    "                fieldnames=[\n",
    "                    RAW_ATTRIBUTES[0],\n",
    "                    \"negative\",\n",
    "                    \"neutral\",\n",
    "                    \"positive\",\n",
    "                ],\n",
    "            )\n",
    "            writer.writeheader()\n",
    "\n",
    "            for date, group in self._engineered_data.groupby(RAW_ATTRIBUTES[0]):\n",
    "                sentiment_prob_sum = [0.0, 0.0, 0.0]\n",
    "\n",
    "                for index, text in enumerate(group[RAW_ATTRIBUTES[1]]):\n",
    "                    utc = group[\"created_utc\"].iloc[index]\n",
    "                    try:\n",
    "                        sentiment_probs = self._predict_sentiment_probabilities(\n",
    "                            text\n",
    "                        )\n",
    "                        sentiment_prob_sum = [\n",
    "                            x + y\n",
    "                            for x, y in zip(sentiment_prob_sum, sentiment_probs)\n",
    "                        ]\n",
    "                    except Exception as e:\n",
    "                        logging.error(\n",
    "                            f\"Error processing text: Date={date}, Value={text}, utc = {utc}\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                average_sentiment_probs = [\n",
    "                    x / len(group[RAW_ATTRIBUTES[1]])\n",
    "                    for x in sentiment_prob_sum\n",
    "                ]\n",
    "                avg_sentiments = {\n",
    "                    RAW_ATTRIBUTES[0]: date,\n",
    "                    \"negative\": average_sentiment_probs[0],\n",
    "                    \"neutral\": average_sentiment_probs[1],\n",
    "                    \"positive\": average_sentiment_probs[2],\n",
    "                }\n",
    "\n",
    "                writer.writerow(avg_sentiments)\n",
    "\n",
    "    def _predict_sentiment_probabilities(self, text) -> List:\n",
    "        inputs = self._tokenizer(text, return_tensors=\"pt\")\n",
    "        outputs = self._model(**inputs)\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        return probabilities.detach().numpy()[0]\n",
    "\n",
    "    def _check_missing_data(self):\n",
    "        data = pd.read_csv(\n",
    "            self._config[\"data\"][self._current_data_index][\"raw\"]\n",
    "        )\n",
    "        missing_values = data.isnull() | (data == \"\")\n",
    "        missing_dates = []\n",
    "        for index, row in missing_values.iterrows():\n",
    "            if (\n",
    "                row[RAW_ATTRIBUTES[0]]\n",
    "                or row[\"created_utc\"]\n",
    "                or row[RAW_ATTRIBUTES[1]]\n",
    "            ):\n",
    "                date = data.at[index, RAW_ATTRIBUTES[0]]\n",
    "                print(\n",
    "                    f\"Row {index + 1} has a missing or empty value. Date: {date}\"\n",
    "                )\n",
    "\n",
    "        for i in range(len(data) - 1):\n",
    "            date1 = datetime.strptime(data.at[i, RAW_ATTRIBUTES[0]], \"%Y-%m-%d\")\n",
    "            date2 = datetime.strptime(\n",
    "                data.at[i + 1, RAW_ATTRIBUTES[0]], \"%Y-%m-%d\"\n",
    "            )\n",
    "\n",
    "            days_diff = (date2 - date1).days\n",
    "            if days_diff > 1:\n",
    "                for j in range(1, days_diff):\n",
    "                    next_date = date1 + timedelta(days=j)\n",
    "                    if next_date.weekday() < 5:\n",
    "                        missing_dates.append(next_date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "        if missing_dates:\n",
    "            print(\"Missing NYSE opening dates:\")\n",
    "            for missing_date in missing_dates:\n",
    "                print(missing_date)\n",
    "        else:\n",
    "            print(\"No missing NYSE opening dates found.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_and_adjust_unix_timestamp(\n",
    "        unix_timestamp: int,\n",
    "    ) -> Union[int, ValueError]:\n",
    "        est = pytz.timezone(\"US/Eastern\")\n",
    "        dt = datetime.fromtimestamp(unix_timestamp, tz=pytz.utc).astimezone(est)\n",
    "        if dt.hour == 9:\n",
    "            return unix_timestamp\n",
    "        elif dt.hour == 8:\n",
    "            return unix_timestamp + 3600\n",
    "        elif dt.hour == 10:\n",
    "            return unix_timestamp - 3600\n",
    "        else:\n",
    "            raise ValueError(\"The provided timestamp is not close to 9 AM EST\")\n",
    "\n",
    "    def _obtain_data(self):\n",
    "        create_file_if_not_exist(\n",
    "            self._config[\"data\"][self._current_data_index][\"raw\"]\n",
    "        )\n",
    "        self._comments = []\n",
    "        nb_comments = 0\n",
    "        end_date = self._end_date_unix\n",
    "        start_date = end_date - 86400\n",
    "        nb_retries = 0\n",
    "        while nb_comments < 500 and nb_retries < 5:\n",
    "            current_comments = list(\n",
    "                self._push_shift.search_comments(\n",
    "                    subreddit=\"wallstreetbets\",\n",
    "                    before=end_date,\n",
    "                    after=start_date,\n",
    "                    fields=[RAW_ATTRIBUTES[1], \"created_utc\"],\n",
    "                    sort=\"asc\",\n",
    "                    limit=100,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if current_comments:\n",
    "                for comment in current_comments:\n",
    "                    comment[RAW_ATTRIBUTES[1]] = comment.pop(\"body\")\n",
    "                self._comments.extend(current_comments)\n",
    "                self._comments = self._keep_unique_values(self._comments)\n",
    "                nb_comments = len(self._comments)\n",
    "                end_date = self._comments[-1][\"created_utc\"]\n",
    "                if end_date < (self._end_date_unix - 86400):\n",
    "                    break\n",
    "            else:\n",
    "                time.sleep(300)\n",
    "                nb_retries += 1\n",
    "\n",
    "    def _filter_raw_data(self):\n",
    "        self._comments = [\n",
    "            {\n",
    "                key: comment[key]\n",
    "                if key != \"permalink\"\n",
    "                else comment.get(key, \"\")\n",
    "                for key in [\"created_utc\", RAW_ATTRIBUTES[1], \"permalink\"]\n",
    "            }\n",
    "            for comment in self._comments\n",
    "            if \"created_utc\" in comment and RAW_ATTRIBUTES[1] in comment\n",
    "        ]\n",
    "\n",
    "        self._comments = [\n",
    "            comment\n",
    "            for comment in self._comments\n",
    "            if comment[RAW_ATTRIBUTES[1]]\n",
    "            not in [\"[deleted]\", \"[removed]\", \"\", None, \"N/A\"]\n",
    "        ]\n",
    "\n",
    "        comments_copy = deepcopy(self._comments)\n",
    "        for item, comment in enumerate(comments_copy):\n",
    "            self._comments[item][\"date\"] = self._current_date\n",
    "\n",
    "        self._comments = sorted(self._comments, key=lambda x: x[\"date\"])\n",
    "        first_date = self._comments[0][RAW_ATTRIBUTES[0]]\n",
    "        if not all(\n",
    "            comment[RAW_ATTRIBUTES[0]] == first_date\n",
    "            for comment in self._comments\n",
    "        ):\n",
    "            raise ValueError(f\"Date are not all the same for current fetch\")\n",
    "\n",
    "    def _save_data_reddit(self):\n",
    "        with open(\n",
    "            self._config[\"data\"][self._current_data_index][\"raw\"],\n",
    "            \"a\",\n",
    "            newline=\"\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as csvfile:\n",
    "            writer = csv.DictWriter(\n",
    "                csvfile,\n",
    "                fieldnames=[\n",
    "                    RAW_ATTRIBUTES[0],\n",
    "                    \"created_utc\",\n",
    "                    RAW_ATTRIBUTES[1],\n",
    "                    \"permalink\",\n",
    "                ],\n",
    "            )\n",
    "            if csvfile.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            for comment in self._comments:\n",
    "                writer.writerow(\n",
    "                    {\n",
    "                        \"date\": comment[\"date\"],\n",
    "                        \"created_utc\": comment[\"created_utc\"],\n",
    "                        RAW_ATTRIBUTES[1]: comment[RAW_ATTRIBUTES[1]],\n",
    "                        \"permalink\": comment[\"permalink\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "\n",
    "class MacroTrends(BaseDataProcessor):\n",
    "    def __init__(\n",
    "        self, specific_config, data_processor_helper, is_input_feature, **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            specific_config, data_processor_helper, is_input_feature, **kwargs\n",
    "        )\n",
    "    def _run_fetch(self):\n",
    "        pass\n",
    "\n",
    "    def _run_common_fetch(self):\n",
    "        pass\n",
    "\n",
    "class FRED(BaseDataProcessor):\n",
    "    def __init__(\n",
    "        self, specific_config, data_processor_helper, is_input_feature, **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            specific_config, data_processor_helper, is_input_feature, **kwargs\n",
    "        )\n",
    "\n",
    "    def _run_fetch(self) -> pd.DataFrame:\n",
    "        request = f\"https://fred.stlouisfed.org/graph/fredgraph.csv?id={self._config['data'][self._current_data_index].get('asset')}\"\n",
    "        request += f\"&cosd={self._config['common']['start_date']}\"\n",
    "        request += f\"&coed={self._config['common']['end_date']}\"\n",
    "        df = pd.read_csv(request, parse_dates=True)\n",
    "        df.rename(\n",
    "            columns={\n",
    "                df.columns[0]: RAW_ATTRIBUTES[0],\n",
    "                df.columns[1]: RAW_ATTRIBUTES[1],\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "class AlphaVantage(BaseDataProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        specific_config,\n",
    "        data_processor_helper,\n",
    "        is_input_feature,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            specific_config, data_processor_helper, is_input_feature, **kwargs\n",
    "        )\n",
    "        self._endpoints = read_json(\"resources/configs/av_api_support.json\")\n",
    "        self._av_key = os.getenv(\"ALPHA_VANTAGE_KEY\")\n",
    "        self._requests_last_minute = deque(maxlen=5)\n",
    "        self._requests_today = 0\n",
    "        self._rate_limiter = RateLimiter(max_calls=100, period=24*3600)\n",
    "\n",
    "    def _fetch_data(self) -> Tuple[List, str]:\n",
    "        if self._requests_today >= 100:\n",
    "            logging.warning(\n",
    "                f\"Not fetching data for {self._config['data'][self._current_data_index]['asset']} \"\n",
    "                f\"on Alpha Vantage. Reach the maximum of 100 on a day\"\n",
    "            )\n",
    "            self._drop_current_asset()\n",
    "\n",
    "        while len(self._requests_last_minute) == 5 and datetime.now() - self._requests_last_minute[0] < timedelta(minutes=1):\n",
    "            time.sleep(5)\n",
    "\n",
    "        with self._rate_limiter:\n",
    "            data = self._config[\"data\"][self._current_data_index].get(\"asset\")\n",
    "            endpoint = next(\n",
    "                k\n",
    "                for k, v in self._endpoints.items()\n",
    "                if data in v[\"supported_symbols\"]\n",
    "            )\n",
    "            input_keys = self._endpoints[endpoint][\"response_helpers\"][\n",
    "                \"input_key_name\"\n",
    "            ]\n",
    "            concatenated_key = \"\".join(input_keys)\n",
    "            self._endpoints[endpoint][\"api_parameters\"][concatenated_key] = data\n",
    "            self._endpoints[endpoint][\"api_parameters\"][\"apikey\"] = self._av_key\n",
    "            response = self._call_rest_api(\n",
    "                self._endpoints[endpoint][\"api_parameters\"]\n",
    "            ).json()\n",
    "\n",
    "            response_data = response[\n",
    "                self._endpoints[endpoint][\"response_helpers\"][\"response\"]\n",
    "            ]\n",
    "\n",
    "\n",
    "        self._requests_last_minute.append(datetime.now())\n",
    "        self._requests_today += 1\n",
    "\n",
    "        return response_data, endpoint\n",
    "\n",
    "    def _run_fetch(self) -> pd.DataFrame:\n",
    "        data_to_write, endpoint = self._fetch_data()\n",
    "        return self._save_to_dataframe(\n",
    "            endpoint,\n",
    "            data_to_write\n",
    "        )\n",
    "\n",
    "    def _obtain_endpoint(self, key: str) -> Dict:\n",
    "        for endpoint in self._endpoints:\n",
    "            if key == endpoint:\n",
    "                return endpoint\n",
    "\n",
    "    def _call_rest_api(self, params: dict) -> requests.Response:\n",
    "        url = \"https://www.alphavantage.co/query\"\n",
    "        return requests.get(url=url, params=params)\n",
    "\n",
    "    def _save_to_dataframe(\n",
    "            self,\n",
    "            endpoint: str,\n",
    "            data: Union[Dict[str, Dict[str, str]], List[Dict[str, str]]]\n",
    "    ) -> pd.DataFrame:\n",
    "        df = pd.DataFrame(columns=self._endpoints[endpoint][\"response_helpers\"][\n",
    "            \"returned_values\"])\n",
    "\n",
    "        if self._endpoints[endpoint][\"response_helpers\"][\"nested_response\"]:\n",
    "            df = self._write_nested_response(df, endpoint, data)\n",
    "        else:\n",
    "            df = self._write_non_nested_response(df, data)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _write_nested_response(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            index: int,\n",
    "            data: Dict[str, Dict[str, str]]\n",
    "    ) -> pd.DataFrame:\n",
    "        for date, point in reversed(data.items()):\n",
    "            row = {\n",
    "                col: point[col]\n",
    "                for col in\n",
    "                self._endpoints[index][\"response_helpers\"][\"returned_values\"][\n",
    "                1:]\n",
    "            }\n",
    "            row[RAW_ATTRIBUTES[0]] = date\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def _write_non_nested_response(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            data: List[Dict[str, str]]\n",
    "    ) -> pd.DataFrame:\n",
    "        for item in reversed(data):\n",
    "            df = pd.concat([df, pd.DataFrame([item])], ignore_index=True)\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
