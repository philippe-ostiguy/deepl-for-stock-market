{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mean_reversion.utils import read_json, obtain_market_dates, clear_directory_content\n",
    "from mean_reversion.config.constants import (\n",
    "    RAW_PATH,\n",
    "    PREPROCESSED_PATH,\n",
    "    ENGINEERED_PATH,\n",
    "    MODEL_PHASES,\n",
    "    MODEL_DATA_PATH,\n",
    "    MODELS_PATH,\n",
    "    TRANSFORMATION_PATH,\n",
    "    PATHS_TO_CREATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mean_reversion.config.model_config import (\n",
    "    PYTORCH_CALLBACKS,\n",
    "    LOSS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31843d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import List, Dict, Text, Any, Optional, Tuple, Union\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaff43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168453d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"warnings.log\", mode=\"w\"),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "logging.basicConfig(\n",
    "    filename=\"errors.log\", level=logging.ERROR, format=\"%(asctime)s %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5177a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigManager:\n",
    "    def __init__(self, file: Optional[Text] = \"config.yaml\") -> None:\n",
    "        self._config = self._load_config(file)\n",
    "        self._add_ticker_dynamically()\n",
    "        self._assign_inputs()\n",
    "        self._config['specific_config'] = {}\n",
    "        self._assign_common_config()\n",
    "        self._config[\"output\"] = self._add_data_files_output(\n",
    "            self._config[\"output\"]\n",
    "        )\n",
    "\n",
    "        self._models_with_hyper = read_json(\n",
    "            \"resources/configs/models_args.json\"\n",
    "        )\n",
    "        self._models_support = read_json(\n",
    "            \"resources/configs/models_support.json\"\n",
    "        )\n",
    "        self.hyperparameters = {}\n",
    "        self.hyperparameters_to_optimize = {}\n",
    "        self._assign_hyperparameters_to_models()\n",
    "        self._validate_num_forecasts()\n",
    "        self._validate_model_metrics()\n",
    "        self._config[\"common\"][\"best_model_path\"] = \\\n",
    "            os.path.join(MODELS_PATH, \"best_model\")\n",
    "        if self._config[\"common\"]['preprocessing']:\n",
    "            clear_directory_content(PREPROCESSED_PATH)\n",
    "        if self._config[\"common\"]['engineering']:\n",
    "            clear_directory_content(ENGINEERED_PATH)\n",
    "            clear_directory_content(MODEL_DATA_PATH)\n",
    "\n",
    "    def _assign_common_config(self):\n",
    "        common_config = self._config[\"hyperparameters\"][\"common\"]\n",
    "        if not \"gradient_clip_val\" in common_config:\n",
    "            self._config[\"hyperparameters\"][\"common\"][\"gradient_clip_val\"] = None\n",
    "\n",
    "        common_config = self._config[\"hyperparameters_optimization\"][\"common\"]\n",
    "        if not \"gradient_clip_val\" in common_config:\n",
    "            self._config[\"hyperparameters\"][\"common\"][\"gradient_clip_val\"] = None\n",
    "\n",
    "    def _add_ticker_dynamically(self):\n",
    "        for sub_dict in self._config[\"inputs\"][\"past_covariates\"]:\n",
    "            data_items = sub_dict.get('data', [])\n",
    "            for item in data_items:\n",
    "                if \"set_dynamically\" in item:\n",
    "                    sub_dict[\"data\"].remove(item)\n",
    "                    subname = item.replace('set_dynamically', '')\n",
    "                    dynamic_data = self._load_data_dynamically(sub_dict[\"source\"], subname)\n",
    "                    sub_dict[\"data\"].extend(dynamic_data)\n",
    "            sub_dict[\"data\"] = list(set(sub_dict[\"data\"]))\n",
    "\n",
    "    def _load_data_dynamically(self, source: str, subname : Optional[str] ='') -> List[str]:\n",
    "        file_path = f\"resources/configs/dynamic_ticker/{source}{subname}_daily.txt\"\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return [line.strip() for line in file]\n",
    "\n",
    "\n",
    "    def get_loss(self, model : str) -> Union[dict,None]:\n",
    "        if self._check_if_argument_exist(model,'loss'):\n",
    "            return deepcopy(self.hyperparameters[model]['loss'])\n",
    "        return None\n",
    "\n",
    "    def _check_if_argument_exist(self, model : str, argument_to_check : str) -> bool:\n",
    "        if self.hyperparameters and argument_to_check in self.hyperparameters[model]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_callbacks(self,\n",
    "                      model,\n",
    "                      hyperparameters_phase : Optional[str] = \"hyperparameters\",\n",
    "                      extra_dirpath : Optional[str] = ''):\n",
    "        pl_trainer_kwargs = deepcopy(self._config[hyperparameters_phase][\"common\"][\"pl_trainer_kwargs\"])\n",
    "        callback_config = deepcopy(self._config[hyperparameters_phase][\"common\"][\"callbacks\"])\n",
    "        if not pl_trainer_kwargs:\n",
    "            del self._config[hyperparameters_phase][\"common\"][\"pl_trainer_kwargs\"]\n",
    "            return\n",
    "\n",
    "        for pl_trainer_argument, pl_trainer_value in pl_trainer_kwargs.items():\n",
    "            if isinstance(pl_trainer_value, list):\n",
    "                pl_trainer_kwargs[pl_trainer_argument] = []\n",
    "                for callback_name in pl_trainer_value:\n",
    "                    if callback_name in callback_config:\n",
    "                        callback_args = callback_config[callback_name]\n",
    "\n",
    "                        if callback_name == \"EarlyStopping\":\n",
    "                            monitor_value = callback_args.get('monitor')\n",
    "                            if monitor_value == 'val_PortfolioReturnMetric':\n",
    "                                callback_args[\n",
    "                                    'monitor'] = monitor_value\n",
    "\n",
    "                                model_checkpoint_args = callback_config.get(\n",
    "                                    \"ModelCheckPoint\", {})\n",
    "                                model_checkpoint_args['monitor'] = monitor_value\n",
    "\n",
    "                        if callback_name == \"ModelCheckPoint\":\n",
    "                            if callback_args.get(\n",
    "                                    'monitor') == 'val_PortfolioReturnMetric':\n",
    "                                callback_args['mode'] = 'max'\n",
    "                                if hyperparameters_phase == \"hyperparameters\" :\n",
    "                                    callback_args['dirpath'] = f'{MODELS_PATH}/{model}'\n",
    "                                else :\n",
    "                                    callback_args['dirpath'] = f'{MODELS_PATH}/hyperparameters_optimization/{model}'\n",
    "                                if extra_dirpath:\n",
    "                                    callback_args['dirpath'] = os.path.join(callback_args['dirpath'],extra_dirpath)\n",
    "                        callback_instance = PYTORCH_CALLBACKS[callback_name](\n",
    "                            **callback_args)\n",
    "                        pl_trainer_kwargs[pl_trainer_argument].append(\n",
    "                            callback_instance)\n",
    "        return pl_trainer_kwargs\n",
    "\n",
    "    def _validate_model_metrics(self):\n",
    "        for metric in self._config[\"common\"][\"metrics_to_choose_model\"]:\n",
    "            if metric not in self._models_support[\"supported_metrics\"]:\n",
    "                raise ValueError(\n",
    "                    f\"Metric {metric} is not a supported metric to\"\n",
    "                    f\"pick the best model\"\n",
    "                )\n",
    "\n",
    "    def _validate_num_forecasts(self):\n",
    "        total_days = len(\n",
    "            obtain_market_dates(\n",
    "                self._config[\"common\"][\"start_date\"],\n",
    "                self._config[\"common\"][\"end_date\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        input_length = self._config[\"hyperparameters\"][\"common\"][\n",
    "            \"max_encoder_length\"\n",
    "        ]\n",
    "        output_length = self._config[\"hyperparameters\"][\"common\"][\n",
    "            \"max_prediction_length\"\n",
    "        ]\n",
    "        train_test_split = self._config[\"common\"][\"train_test_split\"]\n",
    "        validation_forecasts = (\n",
    "            total_days * (train_test_split[1])\n",
    "            - input_length\n",
    "            - output_length\n",
    "            + 1\n",
    "        )\n",
    "        min_forecasts = self._config[\"common\"][\"min_validation_forecasts\"]\n",
    "\n",
    "        if validation_forecasts < min_forecasts:\n",
    "            raise ValueError(\n",
    "                f\"Not enough forecasts for validation,\"\n",
    "                f\"minimum is {min_forecasts} but got {validation_forecasts} \\n\"\n",
    "                f\"In configuration, decrease input_chunk_length, decrease output_chunk_length,\"\n",
    "                f\"increase the date range and/or decrease train_test_split\"\n",
    "            )\n",
    "\n",
    "    def get_model_suggest_type(self, model_name: str, model_argument_type: str = 'hyperparameters',\n",
    "                       keys_only: bool = False) -> dict:\n",
    "        if model_argument_type :\n",
    "            model_args = self._models_with_hyper.get(model_name)[\n",
    "                model_argument_type]\n",
    "        else :\n",
    "            model_args = self._models_with_hyper.get(model_name)\n",
    "        return model_args.keys() if keys_only else model_args\n",
    "\n",
    "    def _assign_inputs(self):\n",
    "        tempo_inputs = self._config[\"inputs\"][\"past_covariates\"]\n",
    "        self._config[\"inputs\"][\"past_covariates\"] = {}\n",
    "        self._config[\"inputs\"][\"past_covariates\"][\"sources\"] = tempo_inputs\n",
    "        self._config[\"inputs\"][\"past_covariates\"][\"common\"] = {}\n",
    "        self._assign_common_inputs()\n",
    "        self._config[\"inputs\"][\"past_covariates\"][\n",
    "            \"common\"\n",
    "        ] = self._add_model_data_path(\n",
    "            self._config[\"inputs\"][\"past_covariates\"][\"common\"], \"input_past\"\n",
    "        )\n",
    "\n",
    "        self._config[\"inputs\"][\"past_covariates\"][\n",
    "            \"sources\"\n",
    "        ] = self._add_data_files_input(\n",
    "            self._config[\"inputs\"][\"past_covariates\"][\"sources\"]\n",
    "        )\n",
    "\n",
    "        self._config[\"inputs\"][\"future_covariates\"][\"common\"] = {}\n",
    "        self._config[\"inputs\"][\"future_covariates\"][\n",
    "            \"common\"\n",
    "        ] = self._add_model_data_path(\n",
    "            self._config[\"inputs\"][\"future_covariates\"][\"common\"],\n",
    "            \"input_future\",\n",
    "        )\n",
    "\n",
    "\n",
    "    def _assign_common_inputs(self):\n",
    "        self._config[\"inputs\"][\"past_covariates\"][\"common\"][\"pickle\"] = {}\n",
    "        self._config[\"inputs\"][\"past_covariates\"][\"common\"][\"pickle\"][\n",
    "            \"features_to_keep\"\n",
    "        ] = f\"{TRANSFORMATION_PATH}/features_to_keep.pickle\"\n",
    "        self._config[\"inputs\"][\"past_covariates\"][\"common\"][\"pickle\"][\n",
    "            \"transformations_for_stationarity\"\n",
    "        ] = f\"{TRANSFORMATION_PATH}/transformations_for_stationarity.pickle\"\n",
    "        self._config[\"inputs\"][\"past_covariates\"][\"common\"][\"pickle\"][\n",
    "            \"transformations_for_scaler\"\n",
    "        ] = f\"{TRANSFORMATION_PATH}/transformations_for_scaler.pickle\"\n",
    "\n",
    "    def _assign_hyperparameters_to_models(self):\n",
    "        self.callbacks = {}\n",
    "        self.callbacks_for_optimization = {}\n",
    "        for model in self._models_with_hyper:\n",
    "            if model == 'common':\n",
    "                continue\n",
    "\n",
    "            self.callbacks[model] = deepcopy(self.get_callbacks(model))\n",
    "            model_args = self._assign_model_hyperparameters(model)\n",
    "            self.hyperparameters[\n",
    "                model] = model_args\n",
    "\n",
    "            if self._config[\"common\"][\"hyperparameters_optimization\"][\"is_optimizing\"]:\n",
    "                self.hyperparameters_to_optimize[\n",
    "                    model] = self._assign_optimization_hyperparameters(model,\n",
    "                                                                       model_args)\n",
    "                self.callbacks_for_optimization[model] = deepcopy(self.get_callbacks(model,\"hyperparameters_optimization\"))\n",
    "\n",
    "\n",
    "    def _assign_model_hyperparameters(self, model):\n",
    "        common_config = deepcopy(self._config[\"hyperparameters\"][\"common\"])\n",
    "        model_specific = deepcopy(self._config[\"hyperparameters\"][\"models\"].get(model,\n",
    "                                                                       {}))\n",
    "        model_keys = self._models_with_hyper[model][\"hyperparameters\"]\n",
    "\n",
    "        model_args = {}\n",
    "        model_args.update(\n",
    "            self._extract_config_values(model_keys.keys(), model_specific))\n",
    "        return self.assign_loss_fct(model_args,common_config)\n",
    "\n",
    "    @staticmethod\n",
    "    def assign_loss_fct(model_args, common_config):\n",
    "        likelihood = []\n",
    "\n",
    "        if \"likelihood\" in common_config and isinstance(\n",
    "                common_config[\"likelihood\"],\n",
    "                list):\n",
    "            likelihood = common_config[\"likelihood\"]\n",
    "\n",
    "        if \"loss\" in model_args and ('Quantile' in model_args[\"loss\"]):\n",
    "            if likelihood:\n",
    "                model_args['loss'] = LOSS[model_args['loss']](\n",
    "                    quantiles=likelihood)\n",
    "\n",
    "            else:\n",
    "                model_args['loss'] = LOSS[model_args['loss']]()\n",
    "\n",
    "            if \"confidence_level\" in common_config and common_config[\n",
    "                'confidence_level'] not in likelihood:\n",
    "                raise ValueError(\n",
    "                    f'Confidence level {common_config[\"confidence_level\"]} must be a value '\n",
    "                    f'in likelihood {likelihood}')\n",
    "\n",
    "        elif \"loss\" in model_args:\n",
    "            model_args['loss'] = LOSS[model_args['loss']]()\n",
    "\n",
    "        return model_args\n",
    "\n",
    "    def _extract_config_values(self, keys : str, config : dict) -> dict:\n",
    "        return {k: config[k] for k in keys if k in config}\n",
    "\n",
    "    def _assign_optimization_hyperparameters(self, model : str, default_args : dict) -> dict:\n",
    "        common_optimization_config =  self._config[\"hyperparameters_optimization\"][\"common\"]\n",
    "        model_specific_optimization_config =  self._config[\"hyperparameters_optimization\"][\"models\"].get(model, {})\n",
    "        model_keys = self._models_with_hyper[model][\"hyperparameters\"]\n",
    "        optimization_args = self._extract_config_values(model_keys.keys(),\n",
    "                                                        common_optimization_config)\n",
    "        optimization_args.update(self._extract_config_values(model_keys.keys(),\n",
    "                                                             model_specific_optimization_config))\n",
    "\n",
    "        missing_keys = set(default_args.keys()) - set(optimization_args.keys())\n",
    "        optimization_args.update({k: default_args[k] for k in missing_keys})\n",
    "\n",
    "        return optimization_args\n",
    "    \n",
    "\n",
    "    def _load_config(self, file: Text) -> dict:\n",
    "        with open(file, \"r\") as yaml_file:\n",
    "            configuration = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "        self._validate_configuration(configuration)\n",
    "        configuration = self._settle_configuration(configuration)\n",
    "\n",
    "        return configuration\n",
    "\n",
    "    def _validate_configuration(self, config: dict) -> None:\n",
    "        model_phase = config[\"common\"][\"model_phase\"]\n",
    "        is_error = True\n",
    "        for phase in MODEL_PHASES:\n",
    "            if model_phase == phase:\n",
    "                is_error = False\n",
    "                break\n",
    "        if is_error:\n",
    "            raise ValueError(f\"model_phase key must be either train or predict\")\n",
    "\n",
    "    def _settle_configuration(self, configuration: dict) -> dict:\n",
    "        common = configuration[\"common\"]\n",
    "        if (\n",
    "            \"max_missing_data\" not in common\n",
    "            or not isinstance(common[\"max_missing_data\"], float)\n",
    "            or not 0 <= common[\"max_missing_data\"] <= 1\n",
    "        ):\n",
    "            configuration[\"common\"][\"max_missing_data\"] = 0.01\n",
    "        return configuration\n",
    "\n",
    "    def _add_data_files_input(\n",
    "        self, data_sources: List[Dict[str, Any]]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        for sub_dict in data_sources:\n",
    "            new_data = []\n",
    "            for value in sub_dict[\"data\"]:\n",
    "                new_data.append(self._add_path_values_in_config(value, \"input\"))\n",
    "            sub_dict[\"data\"] = new_data\n",
    "        return data_sources\n",
    "\n",
    "    def _add_data_files_output(self,all_output: List) -> List:\n",
    "        new_data = {}\n",
    "        final_data = []\n",
    "        for data_per_source in all_output:\n",
    "            new_data_list = []\n",
    "            current_data_source = {}\n",
    "            for target in data_per_source[\"data\"]:\n",
    "                new_data= self._add_path_values_in_config(target, \"output\")\n",
    "                self._add_model_data_path(new_data, \"output\")\n",
    "                new_data_list.append(deepcopy(new_data))\n",
    "            current_data_source['source'] = deepcopy(data_per_source['source'])\n",
    "            current_data_source['data'] = deepcopy(new_data_list)\n",
    "            final_data.append(current_data_source)\n",
    "        return final_data\n",
    "\n",
    "    def _add_model_data_path(self, sub_dict: Dict, data_type: str) -> Dict:\n",
    "        sub_dict[\"model_data\"] = {}\n",
    "        sub_dict[\"model_data\"][\"train\"] = os.path.join(\n",
    "            MODEL_DATA_PATH, f\"{data_type}_train.csv\"\n",
    "        )\n",
    "        sub_dict[\"model_data\"][\"predict\"] = os.path.join(\n",
    "            MODEL_DATA_PATH, f\"{data_type}_predict.csv\"\n",
    "        )\n",
    "        sub_dict[\"model_data\"][\"test\"] = os.path.join(\n",
    "            MODEL_DATA_PATH, f\"{data_type}_test.csv\"\n",
    "        )\n",
    "        sub_dict[\"model_data\"][\"scaler\"] = os.path.join(\n",
    "            MODEL_DATA_PATH, f\"{data_type}_scaler.pkl\"\n",
    "        )\n",
    "        return sub_dict\n",
    "\n",
    "    def _add_path_values_in_config(self, value: str, data_type: str) -> Dict:\n",
    "        return {\n",
    "            \"asset\": value,\n",
    "            \"raw\": os.path.join(RAW_PATH, f\"{value}_input.csv\"),\n",
    "            \"preprocessed\": os.path.join(\n",
    "                PREPROCESSED_PATH, f\"{value}_{data_type}.csv\"\n",
    "            ),\n",
    "            \"engineered\": {\n",
    "                \"train\": os.path.join(\n",
    "                    ENGINEERED_PATH, f\"{value}_{data_type}_train.csv\"\n",
    "                ),\n",
    "                \"predict\": os.path.join(\n",
    "                    ENGINEERED_PATH, f\"{value}_{data_type}_predict.csv\"\n",
    "                ),\n",
    "                \"test\": os.path.join(\n",
    "                    ENGINEERED_PATH, f\"{value}_{data_type}_test.csv\"\n",
    "                ),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _obtain_data_for_current_source(self, current_source: str) -> Dict:\n",
    "        return next(\n",
    "            (\n",
    "                item\n",
    "                for item in self._config[\"inputs\"][\"past_covariates\"][\"sources\"]\n",
    "                if item[\"source\"] == current_source\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def config(self) -> dict:\n",
    "        return self._config\n",
    "\n",
    "    def get_config_for_source(self, source: str, is_input: bool) -> dict:\n",
    "        if is_input:\n",
    "            data_for_source = self._obtain_data_for_current_source(source)\n",
    "            if not data_for_source:\n",
    "                raise ValueError(\n",
    "                    f\"No configuration found for input source: {source}\"\n",
    "                )\n",
    "            data_for_source_common = deepcopy(self._config[\"common\"])\n",
    "            past_covariates_common = deepcopy(\n",
    "                self._config[\"inputs\"][\"past_covariates\"][\"common\"]\n",
    "            )\n",
    "            common_keys = set(data_for_source_common.keys()) & set(\n",
    "                past_covariates_common.keys()\n",
    "            )\n",
    "\n",
    "            if common_keys:\n",
    "                raise ValueError(\n",
    "                    f\"Duplicated keys found in configurations: {common_keys}\"\n",
    "                )\n",
    "            data_for_source[\"common\"] = {\n",
    "                **data_for_source_common,\n",
    "                **past_covariates_common,\n",
    "            }\n",
    "            return data_for_source\n",
    "\n",
    "        else:\n",
    "            # for data in sources['data']:\n",
    "            #     for dataset in self._datasets:\n",
    "            #         shutil.copy(data[\"engineered\"][dataset],\n",
    "            #                 data[\"model_data\"][dataset])\n",
    "            for output in self._config[\"output\"]:\n",
    "                if source == output['source']:\n",
    "                    output_config = output\n",
    "                    output_config[\"common\"] = self._config[\"common\"]\n",
    "                    return output_config\n",
    "\n",
    "    def get_sources(self) -> List[Tuple[str, bool]]:\n",
    "        input_sources = [\n",
    "            (src[\"source\"], True)\n",
    "            for src in self._config[\"inputs\"][\"past_covariates\"][\"sources\"]\n",
    "        ]\n",
    "        output_sources = [(output['source'], False) for output in self._config['output']]\n",
    "        return input_sources + output_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937e799",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def singleton(cls):\n",
    "    instances = {}\n",
    "\n",
    "    def get_instance(*args, **kwargs):\n",
    "        if cls not in instances:\n",
    "            instances[cls] = cls(*args, **kwargs)\n",
    "        return instances[cls]\n",
    "\n",
    "    return get_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2580945",
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class ModelValueRetriver:\n",
    "    def __init__(self, config_manager = ConfigManager()):\n",
    "\n",
    "        self._config = config_manager.config\n",
    "        self._confidence_indexes = ''\n",
    "\n",
    "    @property\n",
    "    def confidence_indexes(self):\n",
    "        if self._confidence_indexes:\n",
    "            return self._confidence_indexes\n",
    "        confidence_level = \\\n",
    "            self._config[\"hyperparameters\"][\"common\"][\n",
    "                \"confidence_level\"] if \"confidence_level\" in \\\n",
    "                                       self._config[\"hyperparameters\"][\n",
    "                                           \"common\"] else .5\n",
    "        likelihood = self._config[\"hyperparameters\"][\"common\"][\n",
    "            \"likelihood\"]\n",
    "        upper_index = likelihood.index(confidence_level)\n",
    "        lower_index = len(likelihood) - 1 - upper_index\n",
    "        return lower_index, upper_index\n",
    "\n",
    "    @confidence_indexes.setter\n",
    "    def confidence_indexes(self, values: tuple):\n",
    "        self._confidence_indexes = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitProject:\n",
    "    def __init__(self, paths_to_create: list = PATHS_TO_CREATE, config : Optional[ConfigManager] = ConfigManager()):\n",
    "        self._paths_to_create = paths_to_create\n",
    "        self._config = config\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create_custom_path(cls) -> Any:\n",
    "        instance = cls()\n",
    "\n",
    "        paths_to_create = []\n",
    "        for model in instance._config.config['hyperparameters']['models']:\n",
    "            paths_to_create.append(os.path.join(MODELS_PATH,model))\n",
    "        for model in instance._config.config['hyperparameters_optimization']['models']:\n",
    "            paths_to_create.append(os.path.join(MODELS_PATH, model))\n",
    "        cls._create_path(paths_to_create)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create_common_path(cls,paths_to_create : list = PATHS_TO_CREATE):\n",
    "        cls._create_path(paths_to_create)\n",
    "\n",
    "    @classmethod\n",
    "    def _create_path(cls, paths_to_create :list):\n",
    "        for path_to_create in paths_to_create:\n",
    "            if not os.path.exists(path_to_create):\n",
    "                os.makedirs(path_to_create)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
