common :
  start_date : '2005-07-01'
  end_date : '2023-12-20'
  max_missing_data : .02
  fetching : False
  preprocessing : False
  engineering : False
  model_phase : 'train'
  min_nb_trades : 20
  train_test_split : [.8,.10,.10]
  min_validation_forecasts : 95
  make_data_stationary: True
  attributes_to_discard: ["volume","split coefficient","adjusted close", "adj close"]
  cross_validation :
    is_running : False
    sliding_windows: 3
  features_engineering :
    check_bell_shape: False
    pca_variance : 1
    is_using_pca : False

  hyperparameters_optimization :
    is_pruning : True
    is_optimizing : False
    nb_trials : 30
    is_using_prev_study : False

  scaling: False
  metrics_to_choose_model :
    - 'ann_return_on_risk'

inputs:
  past_covariates :
  #  - source: 'reddit'
  #    data:
  #      - 'reddit'
  #    size: 100
    - source: 'yahoo'
      data:
#        - 'set_dynamically_50_largest_stocks'
#        - 'NATURAL_GAS'
#        - '3month'
#        - '10year'
#        - '2year'
#        - 'WTI'
#        - 'MSFT'
        - "GC=F"
        - "CC=F"
#        - 'GOOGL'
#        - 'AAPL'
#        - 'AMZN'
#    - source : 'fred'
#      data:
#        - 'set_dynamically'
#        - 'T5YIE'
#        - 'T10YIE'
#        - 'DEXUSNZ'
#        - 'VIXCLS'
  future_covariates :
      data:
        - 'day'
        - 'month'

output :
  - source : 'yahoo'
    data :
      - 'SPY'
      - 'AAPL'


hyperparameters:
  common:
    confidence_level : .7
    batch_size: 128
    epochs: 1
    num_loader_workers : 2
    random_state: 42
    gradient_clip_val : 1
    optimizer : "Ranger"

    likelihood: [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99]
    max_encoder_length: 64
    max_prediction_length: 1
    callbacks :
      EarlyStopping :
        monitor : 'val_PortfolioReturnMetric'
        patience : 20
        verbose : False
        mode : 'max'
      ModelCheckPoint :
        monitor : 'val_PortfolioReturnMetric'
        mode : 'max'
        filename : 'best_model'
        save_top_k : 1
        verbose : True
    pl_trainer_kwargs :
      callbacks:
        - 'EarlyStopping'
        - 'ModelCheckPoint'
  models :
    TemporalFusionTransformer:
      loss: 'QuantileLoss'
      learning_rate: 0.002
      hidden_size: 16
      lstm_layers : 1
      attention_head_size: 1
      dropout: 0.1
      hidden_continuous_size: 8
      #reduce_on_plateau_patience: 5
      outputsize : 7
      weight_decay : 0.02


hyperparameters_optimization:
  common:
    confidence_level : [.5,.6,.7,.8,.9]
    epochs: 60
    gradient_clip_val : [.1,.5,1,5]
    likelihood: [0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95]
    max_encoder_length: [16,32,64,128]
    max_prediction_length: 1
    batch_size: [8,16,32,64,128]
    random_state: 42
    callbacks :
      EarlyStopping :
        monitor : 'val_PortfolioReturnMetric'
        patience : 20
        verbose : False
        mode : 'max'
      ModelCheckPoint :
        monitor : 'val_PortfolioReturnMetric'
        mode : 'max'
        save_top_k : 1
        filename : 'best_model'
        verbose : True
    pl_trainer_kwargs :
      callbacks:
        - 'EarlyStopping'
        - 'ModelCheckPoint'

  models:
    TemporalFusionTransformer:
      loss: ['QuantileLoss','RMSE']
      learning_rate: [0.001,.1]
      hidden_size: [32,64,128,256,512]
      lstm_layers: [1,2,4]
      attention_head_size: [1,2,4,8,16]
      dropout: [0.05,0.3]
      hidden_continuous_size: [32,64,128,256,512]
      weight_decay : [0.0,0.03]


#    TiDEModel:

#      hidden_size: [256,512,1024]
#      temporal_width: [2,4,8,16]
#      temporal_decoder_hidden: [32,64,128]
#      use_layer_norm: [True, False]
#      use_reversible_instance_norm: [True, False]
#    TFTModel:
#      num_attention_heads: [1, 8]
#      hidden_size: [32, 128]
#      hidden_continuous_size: [8, 32]
#      lstm_layers: [1, 3]
#      full_attention : [True, False]
#    TransformerModel:
#      d_model: [4, 16]
#      nhead: [2, 8]
#      num_encoder_layers: [2, 8]
#      num_decoder_layers: [2, 8]
#      dim_feedforward: [64, 256]
#    NBEATSModel:
#      num_blocks: [2, 6]
#      num_stacks: [8, 32]
#      num_layers: [1, 5]
#      layer_widths: [32, 256]
